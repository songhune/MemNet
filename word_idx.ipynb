{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec으로 word_idx 대체하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from memn2n import MemN2N\n",
    "from data_utils import load_task, vectorize_data\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gensim\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def _build_vars(self):\n",
    "        with tf.variable_scope(self._name):\n",
    "            nil_word_slot = tf.zeros([1, self._embedding_size])\n",
    "            A = tf.concat(axis=0, values=[ nil_word_slot, self._init([self._vocab_size-1, self._embedding_size]) ])\n",
    "            C = tf.concat(axis=0, values=[ nil_word_slot, self._init([self._vocab_size-1, self._embedding_size]) ])\n",
    "\n",
    "            self.A_1 = tf.Variable(A, name=\"A\")\n",
    "          #  print('A',A,'A_1',self.A_1)\n",
    "            self.C = []\n",
    "\n",
    "            for hopn in range(self._hops):\n",
    "                with tf.variable_scope('hop_{}'.format(hopn)):\n",
    "                    self.C.append(tf.Variable(C, name=\"C\"))\n",
    "\n",
    "            # Dont use projection for layerwise weight sharing\n",
    "            # self.H = tf.Variable(self._init([self._embedding_size, self._embedding_size]), name=\"H\")\n",
    "\n",
    "            # Use final C as replacement for W\n",
    "            # self.W = tf.Variable(self._init([self._embedding_size, self._vocab_size]), name=\"W\")\n",
    "\n",
    "        self._nil_vars = set([self.A_1.name] + [x.name for x in self.C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nil_word_slot = tf.zeros([1, 40])\n",
    "\n",
    "sess = tf.InteractiveSession()    \n",
    "\n",
    "A = tf.concat(axis=0, values=[nil_word_slot, initializer([38, 40])])\n",
    "print(A.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer=tf.random_normal_initializer(stddev=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c5d84736ba45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "tf.flags.DEFINE_float(\"learning_rate\", 0.01, \"Learning rate for SGD.\")\n",
    "tf.flags.DEFINE_float(\"anneal_rate\", 25, \"Number of epochs between halving the learnign rate.\")\n",
    "tf.flags.DEFINE_float(\"anneal_stop_epoch\", 100, \"Epoch number to end annealed lr schedule.\")\n",
    "tf.flags.DEFINE_float(\"max_grad_norm\", 40.0, \"Clip gradients to this norm.\")\n",
    "tf.flags.DEFINE_integer(\"evaluation_interval\", 10, \"Evaluate and print results every x epochs\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch size for training.\")\n",
    "tf.flags.DEFINE_integer(\"hops\", 3, \"Number of hops in the Memory Network.\")\n",
    "tf.flags.DEFINE_integer(\"epochs\", 100, \"Number of epochs to train for.\")\n",
    "tf.flags.DEFINE_integer(\"embedding_size\", 20, \"Embedding size for embedding matrices.\")\n",
    "tf.flags.DEFINE_integer(\"memory_size\",5, \"Maximum size of memory.\")\n",
    "tf.flags.DEFINE_integer(\"task_id\",3, \"bAbI task id, 1 <= id <= 20\")\n",
    "tf.flags.DEFINE_integer(\"random_state\", None, \"Random state.\")\n",
    "tf.flags.DEFINE_string(\"data_dir\", \"memn2n/data/tasks_1-20_v1-2/en/\", \"Directory containing bAbI tasks\")\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "print(\"Started Task:\", FLAGS.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemN2N(object):\n",
    "    \"\"\"End-To-End Memory Network.\"\"\"\n",
    "    def __init__(self, batch_size, vocab_size, sentence_size, memory_size, embedding_size,\n",
    "        hops=3,\n",
    "        max_grad_norm=40.0,\n",
    "        nonlin=None,\n",
    "        initializer=tf.random_normal_initializer(stddev=0.1),\n",
    "        encoding=position_encoding,\n",
    "        session=tf.Session(),\n",
    "        name='MemN2N'):\n",
    "        \"\"\"Creates an End-To-End Memory Network\n",
    "\n",
    "        Args:\n",
    "            batch_size: The size of the batch.\n",
    "\n",
    "            vocab_size: The size of the vocabulary (should include the nil word). The nil word\n",
    "            one-hot encoding should be 0.\n",
    "\n",
    "            sentence_size: The max size of a sentence in the data. All sentences should be padded\n",
    "            to this length. If padding is required it should be done with nil one-hot encoding (0).\n",
    "\n",
    "            memory_size: The max size of the memory. Since Tensorflow currently does not support jagged arrays\n",
    "            all memories must be padded to this length. If padding is required, the extra memories should be\n",
    "            empty memories; memories filled with the nil word ([0, 0, 0, ......, 0]).\n",
    "\n",
    "            embedding_size: The size of the word embedding.\n",
    "\n",
    "            hops: The number of hops. A hop consists of reading and addressing a memory slot.\n",
    "            Defaults to `3`.\n",
    "\n",
    "            max_grad_norm: Maximum L2 norm clipping value. Defaults to `40.0`.\n",
    "\n",
    "            nonlin: Non-linearity. Defaults to `None`.\n",
    "\n",
    "            initializer: Weight initializer. Defaults to `tf.random_normal_initializer(stddev=0.1)`.\n",
    "\n",
    "            optimizer: Optimizer algorithm used for SGD. Defaults to `tf.train.AdamOptimizer(learning_rate=1e-2)`.\n",
    "\n",
    "            encoding: A function returning a 2D Tensor (sentence_size, embedding_size). Defaults to `position_encoding`.\n",
    "\n",
    "            session: Tensorflow Session the model is run with. Defaults to `tf.Session()`.\n",
    "\n",
    "            name: Name of the End-To-End Memory Network. Defaults to `MemN2N`.\n",
    "        \"\"\"\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._vocab_size = vocab_size\n",
    "        self._sentence_size = sentence_size\n",
    "        self._memory_size = memory_size\n",
    "        self._embedding_size = embedding_size\n",
    "        self._hops = hops\n",
    "        self._max_grad_norm = max_grad_norm\n",
    "        self._nonlin = nonlin\n",
    "        self._init = initializer\n",
    "        self._name = name\n",
    "\n",
    "        self._build_inputs()\n",
    "        self._build_vars()\n",
    "\n",
    "        self._opt = tf.train.AdamOptimizer(learning_rate=self._lr)\n",
    "\n",
    "        self._encoding = tf.constant(encoding(self._sentence_size, self._embedding_size), name=\"encoding\")\n",
    "\n",
    "        # cross entropy\n",
    "        logits = self._inference(self._stories, self._queries) # (batch_size, vocab_size)\n",
    "        #cross_entropy = tf.nn.\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.cast(self._answers, tf.float32), name=\"cross_entropy\")\n",
    "        cross_entropy_sum = tf.reduce_sum(cross_entropy, name=\"cross_entropy_sum\")\n",
    "\n",
    "        # loss op\n",
    "        loss_op = cross_entropy_sum\n",
    "\n",
    "        # gradient pipeline\n",
    "        grads_and_vars = self._opt.compute_gradients(loss_op)\n",
    "        grads_and_vars = [(tf.clip_by_norm(g, self._max_grad_norm), v) for g,v in grads_and_vars]\n",
    "        grads_and_vars = [(add_gradient_noise(g), v) for g,v in grads_and_vars]\n",
    "        nil_grads_and_vars = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if v.name in self._nil_vars:\n",
    "                nil_grads_and_vars.append((zero_nil_slot(g), v))\n",
    "            else:\n",
    "                nil_grads_and_vars.append((g, v))\n",
    "        train_op = self._opt.apply_gradients(nil_grads_and_vars, name=\"train_op\")\n",
    "\n",
    "        # predict ops\n",
    "        predict_op = tf.argmax(logits, 1, name=\"predict_op\")\n",
    "        predict_proba_op = tf.nn.softmax(logits, name=\"predict_proba_op\")\n",
    "        predict_log_proba_op = tf.log(predict_proba_op, name=\"predict_log_proba_op\")\n",
    "\n",
    "        # assign ops\n",
    "        self.loss_op = loss_op\n",
    "        self.predict_op = predict_op\n",
    "        self.predict_proba_op = predict_proba_op\n",
    "        self.predict_log_proba_op = predict_log_proba_op\n",
    "        self.train_op = train_op\n",
    "\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        self._sess = session\n",
    "        self._sess.run(init_op)\n",
    "\n",
    "\n",
    "    def _build_inputs(self):\n",
    "        self._stories = tf.placeholder(tf.int32, [None, self._memory_size, self._sentence_size], name=\"stories\")\n",
    "        self._queries = tf.placeholder(tf.int32, [None, self._sentence_size], name=\"queries\")\n",
    "        self._answers = tf.placeholder(tf.int32, [None, self._vocab_size], name=\"answers\")\n",
    "        self._lr = tf.placeholder(tf.float32, [], name=\"learning_rate\")\n",
    "\n",
    "    def _build_vars(self):\n",
    "        with tf.variable_scope(self._name):\n",
    "            nil_word_slot = tf.zeros([1, self._embedding_size])\n",
    "            A = tf.concat(axis=0, values=[ nil_word_slot, self._init([self._vocab_size-1, self._embedding_size]) ])\n",
    "            C = tf.concat(axis=0, values=[ nil_word_slot, self._init([self._vocab_size-1, self._embedding_size]) ])\n",
    "\n",
    "            self.A_1 = tf.Variable(A, name=\"A\")\n",
    "          #  print('A',A,'A_1',self.A_1)\n",
    "            self.C = []\n",
    "\n",
    "            for hopn in range(self._hops):\n",
    "                with tf.variable_scope('hop_{}'.format(hopn)):\n",
    "                    self.C.append(tf.Variable(C, name=\"C\"))\n",
    "\n",
    "            # Dont use projection for layerwise weight sharing\n",
    "            # self.H = tf.Variable(self._init([self._embedding_size, self._embedding_size]), name=\"H\")\n",
    "\n",
    "            # Use final C as replacement for W\n",
    "            # self.W = tf.Variable(self._init([self._embedding_size, self._vocab_size]), name=\"W\")\n",
    "\n",
    "        self._nil_vars = set([self.A_1.name] + [x.name for x in self.C])\n",
    "\n",
    "    def _inference(self, stories, queries):\n",
    "        with tf.variable_scope(self._name):\n",
    "            # Use A_1 for thee question embedding as per Adjacent Weight Sharing\n",
    "            q_emb = tf.nn.embedding_lookup(self.A_1, queries)\n",
    "            u_0 = tf.reduce_sum(q_emb * self._encoding, 1)\n",
    "       #     print('u_0',u_0)\n",
    "            u = [u_0]\n",
    "          #  print('u',u)\n",
    "            for hopn in range(self._hops):\n",
    "                if hopn == 0:\n",
    "                 #   print('story',stories.shape)\n",
    "                    m_emb_A = tf.nn.embedding_lookup(self.A_1, stories)#(?,50,8,20)\n",
    "                    #각 단어마다 dim=20인 벡터.\n",
    "                  #  print('m_emb+A',m_emb_A)#shape=(?, 50, 8, 20)\n",
    "                    m_A = tf.reduce_sum(m_emb_A * self._encoding, 2)\n",
    "                    '''reduce sum을 하게 되면 dimension이 하나 줄어드는데, \n",
    "                    이말인즉슨 모든 문장에 든 각각의 \"단어\"를 하나로 sum을 한다. '''\n",
    "                 #   print('self._encoding',self._encoding)\n",
    "                #    print('shape m_A//single=hop', m_A)\n",
    "\n",
    "                else:\n",
    "                    with tf.variable_scope('hop_{}'.format(hopn - 1)):\n",
    "                        m_emb_A = tf.nn.embedding_lookup(self.C[hopn - 1], stories)\n",
    "                        m_A = tf.reduce_sum(m_emb_A * self._encoding, 2)#(?,50,20)\n",
    "                     #   print('shape m_A//multiple_hop',m_A)\n",
    "                # hack to get around no reduce_dot\n",
    "                u_temp = tf.transpose(tf.expand_dims(u[-1], -1), [0, 2, 1])\n",
    "                #print('u_temp', u_temp)#shape=(?, 1, 20)\n",
    "                dotted = tf.reduce_sum(m_A * u_temp, 2)\n",
    "                #print('m+A, u_temp',m_A*u_temp)\n",
    "                '''not tensor multiplication'''\n",
    "                # Calculate probabilities\n",
    "                probs = tf.nn.softmax(dotted)\n",
    "               # print('prto', probs)#shape=(?, 50)\n",
    "\n",
    "                probs_temp = tf.transpose(tf.expand_dims(probs, -1), [0, 2, 1])\n",
    "               # print('probs+tmp', probs_temp)#shape=(?, 1, 50)\n",
    "\n",
    "                with tf.variable_scope('hop_{}'.format(hopn)):\n",
    "                    m_emb_C = tf.nn.embedding_lookup(self.C[hopn], stories)\n",
    "                m_C = tf.reduce_sum(m_emb_C * self._encoding, 2)\n",
    "\n",
    "                c_temp = tf.transpose(m_C, [0, 2, 1])\n",
    "               # print('shape c_tmp', c_temp)#c_tmp (?,20,50)\n",
    "\n",
    "                o_k = tf.reduce_sum(c_temp * probs_temp, 2)\n",
    "                #print('o_k',o_k) #shape(?,20)\n",
    "                # Dont use projection layer for adj weight sharing\n",
    "                # u_k = tf.matmul(u[-1], self.H) + o_k\n",
    "\n",
    "                u_k = u[-1] + o_k\n",
    "\n",
    "                # nonlinearity\n",
    "                if self._nonlin:\n",
    "                    u_k = nonlin(u_k)\n",
    "\n",
    "                u.append(u_k)\n",
    "              #  print('u',u)\n",
    "              #  print('u_k',u_k)#u_k shape (?,20)\n",
    "\n",
    "            # Use last C for output (transposed)\n",
    "            with tf.variable_scope('hop_{}'.format(self._hops)):\n",
    "              #  print('C',self.C)\n",
    "                return tf.matmul(u_k, tf.transpose(self.C[-1], [1,0]))\n",
    "\n",
    "    def batch_fit(self, stories, queries, answers, learning_rate):\n",
    "        \"\"\"Runs the training algorithm over the passed batch\n",
    "\n",
    "        Args:\n",
    "            stories: Tensor (None, memory_size, sentence_size)\n",
    "            queries: Tensor (None, sentence_size)\n",
    "            answers: Tensor (None, vocab_size)\n",
    "\n",
    "        Returns:\n",
    "            loss: floating-point number, the loss computed for the batch\n",
    "        \"\"\"\n",
    "        feed_dict = {self._stories: stories, self._queries: queries, self._answers: answers, self._lr: learning_rate}\n",
    "        loss, _ = self._sess.run([self.loss_op, self.train_op], feed_dict=feed_dict)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, stories, queries):\n",
    "        \"\"\"Predicts answers as one-hot encoding.\n",
    "\n",
    "        Args:\n",
    "            stories: Tensor (None, memory_size, sentence_size)\n",
    "            queries: Tensor (None, sentence_size)\n",
    "\n",
    "        Returns:\n",
    "            answers: Tensor (None, vocab_size)\n",
    "        \"\"\"\n",
    "        feed_dict = {self._stories: stories, self._queries: queries}\n",
    "        return self._sess.run(self.predict_op, feed_dict=feed_dict)\n",
    "\n",
    "    def predict_proba(self, stories, queries):\n",
    "        \"\"\"Predicts probabilities of answers.\n",
    "\n",
    "        Args:\n",
    "            stories: Tensor (None, memory_size, sentence_size)\n",
    "            queries: Tensor (None, sentence_size)\n",
    "\n",
    "        Returns:\n",
    "            answers: Tensor (None, vocab_size)\n",
    "        \"\"\"\n",
    "        feed_dict = {self._stories: stories, self._queries: queries}\n",
    "        return self._sess.run(self.predict_proba_op, feed_dict=feed_dict)\n",
    "\n",
    "    def predict_log_proba(self, stories, queries):\n",
    "        \"\"\"Predicts log probabilities of answers.\n",
    "\n",
    "        Args:\n",
    "            stories: Tensor (None, memory_size, sentence_size)\n",
    "            queries: Tensor (None, sentence_size)\n",
    "        Returns:\n",
    "            answers: Tensor (None, vocab_size)\n",
    "        \"\"\"\n",
    "        feed_dict = {self._stories: stories, self._queries: queries}\n",
    "        return self._sess.run(self.predict_log_proba_op, feed_dict=feed_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
