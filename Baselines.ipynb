{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 베이스라인 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Example running MemN2N on a single bAbI task.\n",
    "Download tasks from facebook.ai/babi \"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "from data_utils import load_task, vectorize_data\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from memn2n import MemN2N\n",
    "from itertools import chain\n",
    "from six.moves import range, reduce\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Task: 3\n"
     ]
    }
   ],
   "source": [
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "tf.flags.DEFINE_float(\"learning_rate\", 0.01, \"Learning rate for SGD.\")\n",
    "tf.flags.DEFINE_float(\"anneal_rate\", 25, \"Number of epochs between halving the learnign rate.\")\n",
    "tf.flags.DEFINE_float(\"anneal_stop_epoch\", 100, \"Epoch number to end annealed lr schedule.\")\n",
    "tf.flags.DEFINE_float(\"max_grad_norm\", 40.0, \"Clip gradients to this norm.\")\n",
    "tf.flags.DEFINE_integer(\"evaluation_interval\", 10, \"Evaluate and print results every x epochs\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch size for training.\")\n",
    "tf.flags.DEFINE_integer(\"hops\", 3, \"Number of hops in the Memory Network.\")\n",
    "tf.flags.DEFINE_integer(\"epochs\", 100, \"Number of epochs to train for.\")\n",
    "tf.flags.DEFINE_integer(\"embedding_size\", 20, \"Embedding size for embedding matrices.\")\n",
    "tf.flags.DEFINE_integer(\"memory_size\",5, \"Maximum size of memory.\")\n",
    "tf.flags.DEFINE_integer(\"task_id\",3, \"bAbI task id, 1 <= id <= 20\")\n",
    "tf.flags.DEFINE_integer(\"random_state\", None, \"Random state.\")\n",
    "tf.flags.DEFINE_string(\"data_dir\", \"memn2n/data/tasks_1-20_v1-2/en/\", \"Directory containing bAbI tasks\")\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "print(\"Started Task:\", FLAGS.task_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory size\n",
      "Longest sentence length 8\n",
      "Longest story length 228\n",
      "Average story length 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 6, 33, 28, ...,  0,  0, 39],\n",
       "        [25, 33,  2, ..., 14,  0, 38],\n",
       "        [ 6, 16, 28, ...,  0,  0, 37],\n",
       "        [19,  9, 26, ...,  0,  0, 36],\n",
       "        [15, 21, 28, ...,  0,  0, 35]],\n",
       "\n",
       "       [[ 6, 16, 28, ...,  0,  0, 39],\n",
       "        [19,  9, 26, ...,  0,  0, 38],\n",
       "        [15, 21, 28, ...,  0,  0, 37],\n",
       "        [15, 30, 28, ...,  0,  0, 36],\n",
       "        [15, 30, 28, ...,  0,  0, 35]],\n",
       "\n",
       "       [[15, 21, 28, ...,  0,  0, 39],\n",
       "        [15, 30, 28, ...,  0,  0, 38],\n",
       "        [15, 30, 28, ...,  0,  0, 37],\n",
       "        [ 6, 18, 26, ...,  0,  0, 36],\n",
       "        [25, 30, 28, ...,  0,  0, 35]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[25, 33,  2, ..., 22,  0, 39],\n",
       "        [15, 24,  8, ..., 27,  0, 38],\n",
       "        [ 6, 30, 28, ...,  0,  0, 37],\n",
       "        [15, 23, 31, ...,  0,  0, 36],\n",
       "        [15, 24,  8, ...,  0,  0, 35]],\n",
       "\n",
       "       [[19, 30, 28, ...,  0,  0, 39],\n",
       "        [19,  9, 26, ...,  0,  0, 38],\n",
       "        [ 6, 12, 26, ...,  0,  0, 37],\n",
       "        [ 6, 18, 26, ...,  0,  0, 36],\n",
       "        [ 6, 30, 28, ...,  0,  0, 35]],\n",
       "\n",
       "       [[ 6, 12, 26, ...,  0,  0, 39],\n",
       "        [ 6, 18, 26, ...,  0,  0, 38],\n",
       "        [ 6, 30, 28, ...,  0,  0, 37],\n",
       "        [25, 30, 28, ...,  0,  0, 36],\n",
       "        [19, 21, 28, ...,  0,  0, 35]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# task data\n",
    "train, test = load_task(FLAGS.data_dir, FLAGS.task_id)\n",
    "data = train + test\n",
    "#데이터로부터 사용된 모든 단어를 뽑아내 어순대로 정렬한다.\n",
    "vocab = sorted(reduce(lambda x, y: x | y, (set(list(chain.from_iterable(s)) + q + a) for s, q, a in data)))\n",
    "#데이터로부터 뽑아낸 단어에 인덱스를 매긴다.\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "\n",
    "#story의 최대 길이는 10개\n",
    "max_story_size = max(map(len, (s for s, _, _ in data)))\n",
    "#story의 평균 사이즈는 6개\n",
    "mean_story_size = int(np.mean([ len(s) for s, _, _ in data ]))\n",
    "#story의 문장 길이는 6개\n",
    "sentence_size = max(map(len, chain.from_iterable(s for s, _, _ in data)))\n",
    "#question의 문장 길이는 3개\n",
    "query_size = max(map(len, (q for _, q, _ in data)))\n",
    "#메모리의 크기는 story의 크기와 50 중에 작은 걸로 정해짐\n",
    "#songhune edited: 메모리 자체의 크기를 작게 만듦\n",
    "memory_size = min(FLAGS.memory_size, max_story_size)\n",
    "print('Memory size')\n",
    "# Add time words/indexes\n",
    "for i in range(memory_size):\n",
    "    word_idx['time{}'.format(i+1)] = 'time{}'.format(i+1)\n",
    "\n",
    "vocab_size = len(word_idx) + 1 # +1 for nil word\n",
    "sentence_size = max(query_size, sentence_size) # for the position\n",
    "#sentence 길이에 time word라는 걸 하나 붙이므로(왜 하나 붙이나? 해결됨, timeword라고 표현할 수 있는 부분은 index 하나만 붙이면 되니까)\n",
    "sentence_size += 1  # +1 for time words\n",
    "print(\"Longest sentence length\", sentence_size)\n",
    "print(\"Longest story length\", max_story_size)\n",
    "print(\"Average story length\", mean_story_size)\n",
    "\n",
    "# train/validation/test sets\n",
    "S, Q, A = vectorize_data(train, word_idx, sentence_size, memory_size)\n",
    "#songhune edited 맥스 스토리 사이즈를 넣는다\n",
    "#trainS, valS, trainQ, valQ, trainA, valA = cross_validation.train_test_split(S, Q, A, test_size=.1, random_state=FLAGS.random_state) #this model has been depricated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15, 29, 26, 20,  0,  0,  0, 39],\n",
       "       [15, 33, 28, 26, 11,  0,  0, 38],\n",
       "       [ 6, 33, 28, 26,  4,  0,  0, 37],\n",
       "       [15, 30, 28, 26, 17,  0,  0, 36],\n",
       "       [19, 33, 28, 26, 11,  0,  0, 35]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainS[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing과 ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train에서 실제로 트레이닝 할 데이터를 분리한다. ! 얼마나 트레이닝할때 쓸거냐면 90%를 사용할 것이다. 데이터도 셔플한다.\n",
    "trainS, valS, trainQ, valQ, trainA, valA = train_test_split(S, Q, A, test_size=.1, random_state=FLAGS.random_state)\n",
    "testS, testQ, testA = vectorize_data(test, word_idx, sentence_size, memory_size)\n",
    "\n",
    "'''\n",
    "print(\"########################################################################\")\n",
    "for i in range (1000):\n",
    "    print('the number of i would be',i,'\\n', trainS[i])\n",
    "    print()\n",
    "    print(trainQ[i])\n",
    "    print('###########################')\n",
    "print(\"Training set shape\", trainS.shape)\n",
    "'''\n",
    "print(\"Number of story size of this task\",len(S))\n",
    "\n",
    "# params, 즉, 전체 개수\n",
    "n_train = trainS.shape[0]\n",
    "n_test = testS.shape[0]\n",
    "n_val = valS.shape[0]\n",
    "\n",
    "print(\"Training Size\", n_train)\n",
    "print(\"Validation Size\", n_val)\n",
    "print(\"Testing Size\", n_test)\n",
    "\n",
    "#각각의 정답의 위치를 뱉어낸다.\n",
    "train_labels = np.argmax(trainA, axis=1)\n",
    "test_labels = np.argmax(testA, axis=1)\n",
    "#여기까진 trainset에서의 라벨값\n",
    "val_labels = np.argmax(valA, axis=1)\n",
    "\n",
    "tf.set_random_seed(FLAGS.random_state)\n",
    "batch_size = FLAGS.batch_size\n",
    "\n",
    "batches = zip(range(0, n_train-batch_size, batch_size), range(batch_size, n_train, batch_size))\n",
    "batches = [(start, end) for start, end in batches]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = MemN2N(batch_size, vocab_size, sentence_size,memory_size, FLAGS.embedding_size, max_story_size, session=sess,max_grad_norm=FLAGS.max_grad_norm)\n",
    "    for t in range(1, FLAGS.epochs+1):\n",
    "        # Stepped learning rate\n",
    "        if t - 1 <= FLAGS.anneal_stop_epoch:\n",
    "            anneal = 2.0 ** ((t - 1) // FLAGS.anneal_rate)\n",
    "        else:\n",
    "            anneal = 2.0 ** (FLAGS.anneal_stop_epoch // FLAGS.anneal_rate)\n",
    "        lr = FLAGS.learning_rate / anneal\n",
    "\n",
    "        np.random.shuffle(batches)\n",
    "        total_cost = 0.0\n",
    "        for start, end in batches:\n",
    "            s = trainS[start:end]\n",
    "            q = trainQ[start:end]\n",
    "            a = trainA[start:end]\n",
    "            cost_t = model.batch_fit(s, q, a, lr)\n",
    "            total_cost += cost_t\n",
    "\n",
    "        if t % FLAGS.evaluation_interval == 0:\n",
    "            train_preds = []\n",
    "            for start in range(0, n_train, batch_size):\n",
    "                end = start + batch_size\n",
    "                s = trainS[start:end]\n",
    "                q = trainQ[start:end]\n",
    "                pred = model.predict(s, q)\n",
    "                train_preds += list(pred)\n",
    "\n",
    "            val_preds = model.predict(valS, valQ)\n",
    "            train_acc = metrics.accuracy_score(np.array(train_preds), train_labels)\n",
    "            val_acc = metrics.accuracy_score(val_preds, val_labels)\n",
    "            summary_op = tf.summary.merge_all()\n",
    "\n",
    "            file_writer = tf.summary.FileWriter('./logs',sess.graph)\n",
    "            print('-----------------------')\n",
    "            print('Epoch', t)\n",
    "            print('Total Cost:', total_cost)\n",
    "            print('Training Accuracy:', train_acc)\n",
    "            print('Validation Accuracy:', val_acc)\n",
    "            print('-----------------------')\n",
    "\n",
    "    test_preds = model.predict(testS, testQ)\n",
    "    test_acc = metrics.accuracy_score(test_preds, test_labels)\n",
    "    print(\"Testing Accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
